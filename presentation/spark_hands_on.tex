\documentclass[slidetop,9pt,utf8]{beamer}

\usepackage{CJKutf8}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{ifpdf}
\usepackage{textcomp}
\usepackage{color}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{array}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{pinyin}
\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\usepackage{listingsutf8}
\usepackage{caption}
\usepackage{xspace}

\usetheme{JuanLesPins}
\useoutertheme{infolines}
%\usecolortheme{default}
%\usetheme{Boadilla}
\setbeamerfont{structure}{size*={11}{11}}

\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox[cmyk]{0.43, 0.35, 0.35,0.01}{\parbox{\textwidth}{\hspace{15pt}#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white, singlelinecheck=false, margin=0pt, font={bf,footnotesize}}

\lstset{
   basicstyle=\footnotesize\ttfamily,
   numberstyle=\tiny,          
   numbersep=5pt,              
   tabsize=2,                  
   extendedchars=true,         
   breaklines=true,            
   keywordstyle=\color{red},
   showspaces=false,           
   showtabs=false,             
   xleftmargin=17pt,
   xrightmargin=17pt,
   framexleftmargin=5pt,
   framexrightmargin=5pt,
   framexbottommargin=4pt,
   showstringspaces=false,
   inputencoding=utf8/latin1      
 }

\lstdefinestyle{terminal}
{
  backgroundcolor=\color{black},
  basicstyle=\scriptsize\color{white}\ttfamily
}

\lstdefinestyle{code}
{
  frame=b,
  numbers=left,
  morecomment=[is]{/\*}{\*/}
}

% "define" Scala
\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}

\begin{document}
\begin{CJK}{UTF8}{gbsn}

\AtBeginSection{\frame{\sectionpage}}

\defbeamertemplate{section page}{mine}[1][]{%
  \begin{centering}
    {\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
    \begin{beamercolorbox}[sep=12pt,center]{part title}
      \usebeamerfont{section title}\insertsection\par
    \end{beamercolorbox}
  \end{centering}
}

\setbeamertemplate{section page}[mine]

%--- Page de titre ---
\title[Spark Hands On (PSUG)]{Spark Hands On}
\subtitle{Paris Scala User Group}
\author{Olivier GIRARDOT, Vincent DOBA}
\institute[LT]{Lateral-Thoughts}
\date{May 13, 2015}

\frame{\titlepage}

%--- Sommaire ---
\begin{frame}
  \frametitle{Content}
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Prerequisite}

%--- Prerequisite for the machine ---
\begin{frame}
  \frametitle{Machine Prerequisite}

  \begin{block}{You should have a machine with}
    \begin{itemize}
      \item JDK 6+ (this hands on was developed using Oracle JDK 8) with Scala 2.10+
      \item SBT 13+
      \item Git
      \item An IDE with SBT/Scala support (for instance IDEA IntelliJ with SBT and Scala plugins)
      \item Downloaded Spark 1.3.1 prebuilt for Hadoop 2.6 from \href{http://www.apache.org/dyn/closer.cgi/spark/spark-1.3.1/spark-1.3.1-bin-hadoop2.6.tgz}{official Apache Spark site}
    \end{itemize}
  \end{block}

  \begin{block}{You should}
    \begin{itemize}
      \item Know Scala basis
      \item Be at ease with Scala iterables transformations (map, flatmap, fold)
      \item Be at ease with higher order functions
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{First Spark Run}

  \begin{block}{Perform following commands}
    \begin{lstlisting}[language=bash, style=terminal]
git clone https://github.com/vincentdoba/spark-hands-on.git
cd spark-hands-on
sbt "run-main psug.hands.on.prerequisite.WordCount README.md" 
    \end{lstlisting}
  \end{block}

  \begin{block}{You should obtain}
    \begin{lstlisting}[language=bash, style=terminal]
[info] Running psug.hands.on.prerequisite.WordCount README.md
[error] 15/05/08 17:37:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[info] (,48)
[info] (*,24)
[info] (####,16)
[info] (of,14)
[info] (the,11)
[info] (###,10)
[info] (Exercise,8)
[info] (Description,8)
[info] (Notions,8)
[info] (a,8)
[success] Total time: 25 s, completed 8 mai 2015 17:37:39
    \end{lstlisting}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Words Count Script}

  \lstinputlisting[language=scala, style=code, label=WordCount,caption=/src/main/scala/psug/hands/on/prerequisite/WordCount.scala]{../src/main/scala/psug/hands/on/prerequisite/WordCount.scala}

\end{frame}

\begin{frame}
  \frametitle{Important Notions}

  \begin{block}{Vocabulary}
    \begin{itemize}
      \item \textbf{Spark Conf} Configuration of the Spark Context, here contains application name and master to call
      \item \textbf{Spark Context} Main entry point, represents the connection to a Spark Cluster, used to create RDDs/Load file
      \item \textbf{Resilient Distribued Dataset (RDD)} Spark Dataset on which we can apply actions and transformations
      \item \textbf{Transformations} functions that transform RDD to another RDD
      \item \textbf{Actions} functions that transform an RDD to output data
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Important Notions}

  \begin{block}{Spark Scripting How To}
    \begin{itemize}
      \item Initiate a Spark Context
      \item Load a dataset into a RDD thanks to the Spark Context
      \item Register Transformations on RDD
      \item Apply an Action to transformed RDD
      \item Stop Spark Context
      \item Do whatever you want with data obtained after 
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{SBT Configuration}

  \lstinputlisting[language=scala, style=code, label=WordCount,caption=/build.sbt]{../build.sbt}

\end{frame}

\section{Exercise 1 : Spark basis (SparkContext, RDD, Transformation, Action)}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We have three lists : first list contains the integers from 1 to 75, second list contains the integers from 25 to 100, and the third list contains primes under 100. We want to obtain the sum of square of integers that are under 100 and are not prime.
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item Initialization of Spark Context
      \item Transform Scala lists to Spark RDDs
      \item Discover some transformations on RDDs
      \item Discover first action : sum
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Exercise Code}

  \lstinputlisting[language=scala, style=code, label=Exercise1,caption=/src/main/scala/psug/hands/on/exercise01/SumOfSquareOfNonPrimeNumbers.scala]{../src/main/scala/psug/hands/on/exercise01/SumOfSquareOfNonPrimeNumbers.scala}

\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions (1)}

  \begin{lstlisting}[label=InitSparkContext, caption=Init Spark Context, language=scala, style=code]
val conf = new SparkConf()
  .setMaster("local")
  .setAppName("applicationName")
val sparkContext = new SparkContext(conf)
  \end{lstlisting}

  \begin{lstlisting}[label=ListToRdd, caption=Load List as RDD, language=scala, style=code]
val scalaList:List[T] = List(...)
val rdd:RDD[T] = sparkContext.makeRDD(scalaList)
  \end{lstlisting}

\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions (2)}

  \begin{lstlisting}[label=RDDTransformation, caption=Apply transformation to RDD, language=scala, style=code]
val newRdd:RDD[T] = oldRdd.map(x => x)
  \end{lstlisting}

  \begin{lstlisting}[label=RDDAction, caption=Apply action to RDD, language=scala, style=code]
val result:Int = rdd[Int].sum()
  \end{lstlisting}

  \begin{lstlisting}[label=StopSparkContext, caption=Stop Spark Context, language=scala, style=code]
sparkContext.stop()
  \end{lstlisting}

\end{frame}

\begin{frame}

  \frametitle{New Transformations and Actions}

  \begin{block}{Transformations}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Transformation} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{union} & \textit{rdd1.union(rdd2)} & merge the two rdds \\ \hline
        \textbf{distinct} & \textit{rdd1.distinct()} & remove duplicates from rdd \\ \hline
        \textbf{subtract} & \textit{rdd1.substract(rdd2)} & remove values from rdd2 of rdd1 \\ \hline
        \textbf{map} & \textit{rdd1.map(f: (T) =\textgreater\xspace U)} & apply function f to elements of rdd1 \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

  \begin{block}{Actions}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Action} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{sum} & \textit{rdd1.sum()} & sum the elements of RDD \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Command and Result}

  \begin{block}{Command}
    \begin{lstlisting}[language=bash, style=terminal]
sbt "run-main psug.hands.on.exercise01.SumOfSquaresOfNonPrimeNumbers"
    \end{lstlisting}
  \end{block}

  \begin{block}{Result}
    \begin{lstlisting}[language=bash, style=terminal]
[info] The sum of square of numbers that are not prime and are under 100 is 272554.0
[success] Total time: 9 s, completed 8 mai 2015 21:23:22
    \end{lstlisting}
  \end{block}

\end{frame}

\section{Exercise 2 : Key/Values RDD, File Loading}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We have a text file containing the list of French departments. Which are the departments whose name contains Seine ? And Loire ? And Garonne ? And Rhône ?
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item Load a file using a Spark Context
      \item New Actions and Transformations
      \item How to transform an RDD to a Key/Value RDD
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Exercise Code}

  \lstinputlisting[language=scala, style=code, label=Exercise2,caption=/src/main/scala/psug/hands/on/exercise02/DepartmentsByRiver.scala]{../src/main/scala/psug/hands/on/exercise02/DepartmentsByRiver.scala}

\end{frame}

\begin{frame}[fragile]
  \frametitle{departements.txt File}

  \begin{verbatim}
Ain,01
Aisne,02
Allier,03
Alpes-de-Haute-Provence,04
Hautes-Alpes,05
Alpes-Maritimes,06
Ardèche,07
Ardennes,08
Ariège,09
Aube,10
Aude,11
Aveyron,12
Bouches-du-Rhône,13
Calvados,14
Cantal,15
Charente,16
Charente-Maritime,17
...
  \end{verbatim}

\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions}

  \begin{lstlisting}[label=LoadTextFile, caption=Load Text File, language=scala, style=code]
val rdd:RDD[String] = sparkContext.textFile("filePath")
  \end{lstlisting}

  \begin{lstlisting}[label=TransformToKeyValue, caption=Transform RDD to a Key/Value RDD, language=scala, style=code]
val keyValueRdd1:RDD[(K,V)] = oldRdd[T].map(f: (T) => (K,V))
val keyValueRdd2:RDD[(K,V)] = oldRdd[T]
  .flatMap(f: (T) => TraversableOnce[(K,V)])
  \end{lstlisting}

\end{frame}

\begin{frame}

  \frametitle{New Transformations and Actions}

  \begin{block}{Transformations}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Transformation} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{filter} & \textit{rdd1.filter(f: (T) =\textgreater\xspace boolean)} & filter by function f \\ \hline
        \textbf{flatMap} & \textit{rdd1.flatMap(f: (T) =\textgreater\xspace TraversableOnce[U])} & apply function f and flatten \\ \hline
        \textbf{groupByKey} & \textit{rdd1[(K,V)].groupByKey()} & create an RDD containing tuples (K, Seq(V)) \\ \hline
        \textbf{sortByKey} & \textit{rdd1[(K,V)].sortByKey()} & sort rdd1 by key \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

  \begin{block}{Actions}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Action} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{collect} & \textit{rdd1.collect()} & retrieve all elements of RDD as Array \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Command and Result}

  \begin{block}{Command}
    \begin{lstlisting}[language=bash, style=terminal]
sbt "run-main psug.hands.on.exercise02.DepartmentsByRiver"
    \end{lstlisting}
  \end{block}

  \begin{block}{Result}
    \begin{lstlisting}[language=bash, style=terminal]
[info] Les departements dont le nom contient Garonne sont Haute-Garonne, Lot-et-Garonne, Tarn-et-Garonne
[info] Les departements dont le nom contient Loire sont Indre-et-Loire, Loire, Haute-Loire, Loire-Atlantique, Loiret, Maine-et-Loire, Saone-et-Loire
[info] Les departements dont le nom contient Rhone sont Bouches-du-Rhone, Rhone
[info] Les departements dont le nom contient Seine sont Seine-Maritime, Seine-et-Marne, Hauts-de-Seine, Seine-Saint-Denis
[success] Total time: 8 s, completed 8 mai 2015 22:03:26
    \end{lstlisting}
  \end{block}

\end{frame}

\section{Exercise 3 : Spark SQL Context, JSON Loading, DataFrames}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We have a JSON file containing the list of French cities with population. What is the total population of France ?
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item Initialize Spark SQL Context
      \item Load DataFrames from JSON File with a Spark SQL Context
      \item First Actions and Transformations on DataFrames
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Exercise Code}

  \lstinputlisting[language=scala, style=code, label=Exercise3,caption=/src/main/scala/psug/hands/on/exercise03/TotalPopulation.scala]{../src/main/scala/psug/hands/on/exercise03/TotalPopulation.scala}

\end{frame}

\begin{frame}[fragile]
  \frametitle{demographie\_par\_commune.json File}

  \begin{verbatim}
{"CodeInsee":"01001",[...],"Population":784,[...]}
{"CodeInsee":"01002",[...],"Population":221,[...]}
{"CodeInsee":"01004",[...],"Population":13835,[...]}
{"CodeInsee":"01005",[...],"Population":1616,[...]}
{"CodeInsee":"01006",[...],"Population":116,[...]}
{"CodeInsee":"01007",[...],"Population":2362,[...]}
{"CodeInsee":"01008",[...],"Population":729,[...]}
{"CodeInsee":"01009",[...],"Population":340,[...]}
{"CodeInsee":"01010",[...],"Population":994,[...]}
{"CodeInsee":"01011",[...],"Population":363,[...]}
{"CodeInsee":"01012",[...],"Population":302,[...]}
{"CodeInsee":"01013",[...],"Population":163,[...]}
{"CodeInsee":"01014",[...],"Population":3476,[...]}
{"CodeInsee":"01015",[...],"Population":480,[...]}
{"CodeInsee":"01016",[...],"Population":399,[...]}
{"CodeInsee":"01017",[...],"Population":421,[...]}
{"CodeInsee":"01019",[...],"Population":20,[...]}
...
  \end{verbatim}
\end{frame}

\begin{frame}
  \frametitle{New Notions (1)}

  \begin{block}{Data Frame}
    \begin{itemize}
      \item Equivalent of RDD for structured data, DataFrame class extends RDD Api
      \item Can be seen as a SQL Table (columns and set of rows)
      \item Has specific Transformations/Actions
    \end{itemize}
  \end{block}

  \begin{block}{Row}
    \begin{itemize}
      \item Element of a Data Frame.
    \end{itemize}
  \end{block}

  \begin{block}{Column}
    \begin{itemize}
      \item Part of the structure of a Data Frame.
    \end{itemize}
  \end{block}

  \begin{block}{SQL Context}
    \begin{itemize}
      \item Equivalent for Data Frame of Spark Context for RDD
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions (2)}

  \begin{lstlisting}[label=SqlContextInitialization, caption=Init Spark SQL Context, language=scala, style=code]
val sqlContext = new SQLContext(sparkContext)
  \end{lstlisting}

  \begin{lstlisting}[label=JsonFileLoading, caption=Load a Data Frame from JSON file, language=scala, style=code]
val input:DataFrame = sqlContext.jsonFile(filePath)
  \end{lstlisting}

  \begin{lstlisting}[label=AggregationByFunction, caption=Aggregation by a SQL Function, language=scala, style=code]
import org.apache.spark.sql.functions._
val newDataFrame:DataFrame = oldDataFrame.agg(sum("ColumnName"))
  \end{lstlisting}

\end{frame}

\begin{frame}

  \frametitle{Data Frame Transformations and Actions}

  \begin{block}{Transformations}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Transformation} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{filter} & \textit{df1.filter( \newline  "SQLConditionStatement" \newline )} & filter by applying SQL statement \\ \hline
        \textbf{agg} & \textit{df1.agg(f("columnName"))} & aggregate depending of f function. f can be sum, avg, max, min, count\\ \hline
      \end{tabular}
    \end{center}
  \end{block}

  \begin{block}{Actions}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Action} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{first} & \textit{rdd1.first()} & return the first element of RDD (in case of Data Frame, return first row) \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Command and Result}

  \begin{block}{Command}
    \begin{lstlisting}[language=bash, style=terminal]
sbt "run-main psug.hands.on.exercise03.TotalPopulation"
    \end{lstlisting}
  \end{block}

  \begin{block}{Result}
    \begin{lstlisting}[language=bash, style=terminal]
[info] La France compte 64612939 habitants
[success] Total time: 9 s, completed 8 mai 2015 23:27:30
    \end{lstlisting}
  \end{block}

\end{frame}

\section{Exercise 4 : Transformations/Actions on DataFrames, Join}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We have a JSON file containing the list of French cities with population, surface and department code and a text file containing departement code and name. We want the name of the ten densest French department, ordered by density.
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item Joining two RDDs
      \item Transform Data Frame to RDD
      \item New Actions and Transformations on DataFrames
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Exercise Code}

  \lstinputlisting[language=scala, style=code, label=Exercise4,caption=/src/main/scala/psug/hands/on/exercise04/DensestDepartments.scala]{../src/main/scala/psug/hands/on/exercise04/DensestDepartments.scala}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Data files}

  \begin{block}{demographie\_par\_commune.json File}
    \begin{verbatim}
{[...],"Departement":"01",[...],"Population":784,"Superficie":16,[...]}
{[...],"Departement":"01",[...],"Population":221,"Superficie":9,[...]}
{[...],"Departement":"01",[...],"Population":13835,"Superficie":25,[...]}
{[...],"Departement":"01",[...],"Population":1616,"Superficie":16,[...]}
{[...],"Departement":"01",[...],"Population":116,"Superficie":6,[...]}
...
    \end{verbatim}
  \end{block}

  \begin{block}{departements.txt File}
    \begin{verbatim}
Ain,01
Aisne,02
Allier,03
Alpes-de-Haute-Provence,04
Hautes-Alpes,05
...
    \end{verbatim}
  \end{block}


\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions}

  \begin{lstlisting}[label=ExtractValueFromARow, caption=Extract value from a Row, language=scala, style=code]
def extractor(row:Row):(String, Double) => (row.getString(columnIndex1), row.getDouble(columnIndex2)))
  \end{lstlisting}

  \begin{lstlisting}[label=DataFrameToRDD, caption=Transform a Data Frame to RDD, language=scala, style=code]
val rdd = dataFrame.map(row => f(row))
  \end{lstlisting}

  \begin{lstlisting}[label=changeNameSelectedColumn, caption=Change name of a column, language=scala, style=code]
val newDataFrame = oldDataFrame.select(oldDataFrame("columnName").as("newColumnName"))
  \end{lstlisting}

\end{frame}

\begin{frame}

  \frametitle{New Data Frame Transformations}

  \begin{block}{Transformations}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Transformation} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{select} & \textit{df1.select("column1", "column2", "column3")} & Transform data frame to a new data frame with columns specified as argument \\ \hline
        \textbf{groupBy} & \textit{df1.groupBy(\newline"columnName"\newline)} & Similar to GroupByKey, group by elements of column columnName \\ \hline
        \textbf{join} & \textit{rdd1[(K,V1)]\newline  .join(rdd2[(K,V2)])} & Join the two RDD to one RDD, new value is the tuple of the two old values \\ \hline
        \textbf{sortBy} & \textit{rdd1[T]\newline  .sortBy(f: (T) =\textgreater\xspace K )} & Sort by the function f \\ \hline
        \textbf{values} & \textit{rdd1.values()} & Transform a Key/Value RDD to a RDD containing only values \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}

  \frametitle{New RDD Action}

  \begin{block}{Actions}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \rowcolor{gray} \textbf{Action} & \textbf{Usage} & \textbf{Description} \\ \hline
        \textbf{take} & \textit{rdd1.take(n:Int)} & return n elements of RDD (in case of Data Frame, return n rows) \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Command and Result}

  \begin{block}{Command}
    \begin{lstlisting}[language=bash, style=terminal]
sbt "run-main psug.hands.on.exercise04.DensestDepartments"
    \end{lstlisting}
  \end{block}

  \begin{block}{Result}
    \begin{lstlisting}[language=bash, style=terminal]
[info] Les departements les plus densement peuples sont Paris, Hauts-de-Seine, Seine-Saint-Denis, Val-de-Marne, Val-d'Oise, Essonne, Yvelines, Rhone, Nord, Bouches-du-Rhone
[success] Total time: 17 s, completed 9 mai 2015 01:17:31
    \end{lstlisting}
  \end{block}

\end{frame}

\section{Exercise 5 : Filter Transformations on DataFrames}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We have a JSON file containing the list of French cities with city name, population, surface, number of executives, number of farmers, number of workers, number of employees. We want the name, the density, the percentage of executives, the percentage of farmers, the percentage of workers and the percentage of employees of the densest city in France and also if this city has more than 5000 inhabitants or not.
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item New Transformations
      \item Treat rows that contains null values
    \end{itemize}
  \end{block}

\end{frame}

\begin{frame}
  \frametitle{Exercise Code}

  \lstinputlisting[language=scala, style=code, label=Exercise5,caption=/src/main/scala/psug/hands/on/exercise05/DensestCityDemography.scala]{../src/main/scala/psug/hands/on/exercise05/DensestCityDemography.scala}

\end{frame}

\begin{frame}

  \frametitle{demographie\_par\_commune.json File Data Structure}

  \begin{block}{Interesting Columns}
    \begin{tabular}{|l|l|l|l|}
          \hline 
          \rowcolor{gray} \textbf{Column Name} & \textbf{Type} & \textbf{Description} & \textbf{Example} \\ \hline
          \textbf{Commune} & \textit{String} & City's name & "Ambronay" \\ \hline
          \textbf{Population} & \textit{Double} & Number of inhabitants & 116 \\ \hline
          \textbf{Superficie} & \textit{Double} & Surface ($km^{2}$) & 9 \\ \hline
          \textbf{Agriculteurs} & \textit{Double} & Number of farmers & 16 \\ \hline
          \textbf{Cadresetprofessionssupérieurs} & \textit{Double} & Number of executives & 80 \\ \hline
          \textbf{Employés} & \textit{Double} & Number of employees & 117 \\ \hline
          \textbf{Ouvriers} & \textit{Double} & Number of workers & 92 \\ \hline
    \end{tabular}
  \end{block}

\begin{block}{Why Double instead of Long ?}
Long values (such as number of inhabitants) are treated as Double in order to be processed in a machine learning Spark integrated algorithm.
\end{block}

\end{frame}

\begin{frame}[fragile]

  \frametitle{City class structure}

  \begin{lstlisting}[label=CityClassOverview, caption=psug.hands.on.exercise05.City class overview, language=scala, style=code]
case class City(name:String, category:Double, features: List[Double]) extends Serializable
  \end{lstlisting}

  \begin{block}{Category}
    Contains 1 if the City has more than 5000 inhabitants, else 0
  \end{block}

  \begin{block}{Features}
    \begin{center}
      \begin{tabular}{|l|l|l|l|}
            \hline 
            \rowcolor{gray} \textbf{Position} & \textbf{Name} \\ \hline
            \textbf{features(0)} & Density \\ \hline
            \textbf{features(1)} & Percentage of executives \\ \hline
            \textbf{features(2)} & Percentage of employee \\ \hline
            \textbf{features(3)} & Percentage of workers \\ \hline
            \textbf{features(4)} & Percentage of farmers \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{New Notions}

  \begin{lstlisting}[label=DropNullValue, caption=Drop row containing null values, language=scala, style=code]
val newDataFrame = oldDataFrame.na.drop()
  \end{lstlisting}

  \begin{lstlisting}[label=FillNullValue, caption=Fill row containing null values, language=scala, style=code]
val newDataFrame = oldDataFrame.na.fill(value)
  \end{lstlisting}

\end{frame}

\begin{frame}

  \frametitle{New Data Frame Transformations}

  \begin{block}{Transformations}
    \begin{center}
      \begin{tabular}{|m{2.1cm}|m{3.5cm}|m{5cm}|}
        \hline 
        \textbf{where} & \textit{df1.where( \newline  "SQLConditionStatement" \newline )} & Equivalent to filter, filter by applying SQL statement \\ \hline
      \end{tabular}
    \end{center}
  \end{block}

\end{frame}

\begin{frame}[fragile]
  \frametitle{Command and Result}

  \begin{block}{Command}
    \begin{lstlisting}[language=bash, style=terminal]
sbt "run-main psug.hands.on.exercise05.DensestCityDemography"
    \end{lstlisting}
  \end{block}

  \begin{block}{Result}
    \begin{lstlisting}[language=bash, style=terminal]
[info] La ville la plus densement peuplee de France est Levallois-Perret avec une densite de 32126.0 habitants par km2, 
[info] elle a plus de 5000 habitants, 
[info] et est constituee a 25.0 % de cadres, 10.0 % d'employes, 3.0 % d'ouvriers et 0.0 % de fermiers
[success] Total time: 18 s, completed 9 mai 2015 02:37:52
    \end{lstlisting}
  \end{block}

\end{frame}


\section{Exercise 6 : Saving JSON file}

\begin{frame}
  \frametitle{Presentation of Exercise}

  \begin{block}{Statement}
    We continue the previous exercise (Exercise 5). We have an RDD[City] as defined at the end of exercise 5 (copy/paste code from exercise 5 to start this exercise). We want to normalize the features of each city, meaning applying the following formula for each feature :
\begin{center} 
    $normalizedCity.features(i) = \frac{city.features(i) - min(allCities.map(\_.features(i)))}{max(allCities.map(\_.features(i))) - min(allCities.map(\_.features(i)))}$
\end{center}
And save these normalized cities in a JSON file normalized\_cities.json
  \end{block}

  \begin{block}{What you will learn}
    \begin{itemize}
      \item Cache RDD computation to reuse it several times
      \item Aggregation on RDD
      \item Save JSON file
    \end{itemize}
  \end{block}

\end{frame}

\section{Exercise 7 : Machine Learning}


\end{CJK}
\end{document}
